Introduction
============

SPARQL Benchmarker is a set of simple command line tools that can be used to test
SPARQL systems accessible via HTTP

Benchmarking
------------

The benchmark command is used to benchmark the performance of a SPARQL system.

On *nix systems you can invoke the benchmarker like so:

  ./benchmark [options]

On Windows systems you can invoke the benchmarker like so:

  benchmark.bat [options]

Or you can invoke the jar directly using Java as follows:

  java -jar sparql-query-bm-cli.jar [options]

To see full usage summary run with the -h or --help option

Soak Testing
------------

The soak command is used to stress test a SPARQL system by continually running tests against it
for some user defined period of time.

On *nix systems you can invoke the soak tester like so:

  ./soak [options]
  
On Windows systems you can invoke the soak tester like so:

  soak.bat [options]
  
Or you can invoke the jar directly using Java as follows:

  java -cp sparql-query-bm-cli.jar net.sf.sparql.query.benchmarking.cmd.SoakCmd [options]
  
To see full usage summary run with the -h or --help option

General Notes
=============

With sanity checking enabled (as it is by default for all commands) a number of tests will be issued 
prior to any benchmarking to determine if the given endpoint is up and available for use.  There are 
3 tests currently and by default at least 2 must pass for the benchmarker to run.  Set -s 0 to disable
these checks.

If errors occur they will be logged using log4j, the --logging argument will enable log4j logging
to stdout.  Alternatively the --log-file option sets a log file for log output.  The --debug or --trace
options are used to increase the log level to DEBUG/TRACE which is useful for debugging the HTTP
communications going on.

Currently the following result formats are supported:
- ASK results
 - application/sparql-results+xml (SPARQL XML)
- CONSTRUCT/DESCRIBE results
 - application/rdf+xml (RDF XML)
 - text/turtle (Turtle)
 - text/plain (NTriples)
 - application/rdf+json (RDF JSON, Talis specification [1])
- SELECT results
 - application/sparql-results+xml (SPARQL XML)
 - application/sparql-results+json (SPARQL JSON)
 - text/tab-separated-values (SPARQL TSV)
 - text/csv (SPARQL CSV)
 
Authentication uses ARQ's HTTP authentication API, this supports standard HTTP authentication methods
e.g. Basic, Digest, form based login such as that provided by Apache mod_auth_form.  When only the
--username and --password options are specified then standard HTTP authentication is used, if the
--form-url option is specified then form based login is used.
 
The --preemptive-auth option may be used to enabled preemptive authentication when using standard HTTP
authentication, note that this only works if the remote server is using Basic authentication, otherwise it
has no effect.
 
Benchmarking Methodology
========================

Benchmarks are run on Query Mixes, a Query Mix contains one/more queries, a mix may repeat the same query
several times if desired.  The actual benchmark consists of N runs of this query mix (default N=25),
number of runs may be configured using the -r or --runs options.  By default queries are run in a random
order for each run to try and avoid the SPARQL engine learning the pattern of queries and aggressively
caching and thus gaming the benchmark.  Randomization may be turned off if desired using the --norand
option.  Prior to the actual benchmarked runs N warmup runs are performed (default N=5) to exercise the
system such that it may ramp up into a hot running state that will show optimum performance.  For some
systems this may be irrelevant or may require more runs so you can configure this with the -w or
--warmups option.

Each run records both the response time and the runtime for that run, response time is considered to
be the time from when the query is issued to when the HTTP response starts.  The runtime is calculated 
as the total time from issuing the query to the HTTP response coming back and for it to be parsed on the 
client such that we can count the number of results.  This is in line with the methodology outlined 
in other SPARQL benchmarking reports such as [2] by Revelytix. Runtime is individual to a query, 
query mix runtimes are a summation of the runtimes of each individual query rather than the time
to run the entire mix of queries i.e. the runtime does not include any overhead of the actual 
benchmarking process as far as is possible.

In some cases we have seen that the overhead of serializing results back over HTTP may significantly 
exceed the time taken for the system to actually execute the query, we provide several options for
configuring the format requested so you may wish to choose whichever format is fastest for the
endpoint you are benchmarking.  See preceding notes on options for supported formats.  You may also
wish to skip the counting of results or apply a fixed limit on the number of results to each query, see
the list of options for the relevant options.

As the benchmarker runs it reports statistics for each query mix run.  Once all runs have completed
it discards the best and worst N results as outliers (this defaults to N=1 but is configurable) before
calculating statistics both per query and for the complete query mix.

Currently statistics calculated are as follows:
-Total Response Time
-Average Response Time (Arithmetic Mean)
-Total Runtime
-Average Runtime (Arithmetic Mean)
-Average Runtime (Geometric Mean)
-Minimum Runtime
-Maximum Runtime
-Runtime Variance (Population Variance)
-Runtime Standard Deviation (Population Standard Deviation)
-Queries per Second
-Queries per Hour
-Query Mixes per Hour

If performing a multi-threaded benchmark then additional stats will be included, see the subsequent
section for details.

As well as printing information to stdout the benchmarker will also generate a CSV and an XML file at the end of
the benchmarking process.  These file contains overall statistics as well as run and per query statistics.
Additionally they may also gather environmental and settings information for the runs.  There are options 
available to customise the filenames, whether they are generated at all and whether they are allowed to 
overwrite existing files with the same names. 

Single vs Multi-Threaded Benchmarking
-------------------------------------

By default the benchmarker runs in single threaded mode so only a single query will ever be running
at one time.  The benchmarker can be made to run in multi-threaded mode using the -p or --parallel
option to set the number of threads.  When running in this mode the entire query mix is run in
parallel so there may be up to N mixes and thus N queries running at one time where N is the number 
of threads specified by the user.

Please note that the random delay between queries may mean that there are less than N queries
actually running so you may wish to disable delays by using -d 0 or --delay 0 in order to ensure that
there are always N queries running when performing a multi-threaded benchmark.  Also you may wish to
use the --norand option so that mixes are more likely to be running the same query simultaneously
though this will not necessarily simulate real world usage of a system very well.

Multi-threaded benchmarking allows you to put additional strain on your system so you may see
different performance figures versus single-threaded benchmarking.

When performing multi-threaded benchmarking additional stats will be generated, these are as follows:
-Actual Runtime
-Actual Average Runtime (Arithmetic Mean)
-Actual Queries per Second
-Actual Queries per Hour
-Actual Query Mixes per Hour

These are equivalent to the single-threaded metrics except that they account for the parallelization
of operations, typically these metrics will show better figures than the single threaded figures
though this may depend on the system and the query.  Multi-threaded stats include a tiny fraction of
benchmarking overhead which single-threaded stats do not so the figures may be higher for some
cases.

Publishing Results
------------------

It is strongly suggested that anyone using this tool to publish results follows best practices
with regards to transparency.  We recommend disclosing technical specifications of the system
being benchmarked and the hardware it was run on, where the benchmarker was run relative to the
benchmarked system (typically we'd expect the two to be run on the same machine or on machines
in the same LAN) and the full output of the benchmarker (ideally either the CSV/XML output).

Operation Mixes
===============

This tool includes several operation mixes to get you started, you can create an operation mix for any
set of operations you wish to test with.

- You can find SP2B queries [3] under queries\sp2b.txt
    SP2B without Query 5a which is typically the hardest and may not be runnable on some systems for
    larger dataset sizes can be found under queries\sp2b_reduced.txt
- You can find LUBM queries [4] under queries\lubm.txt
- You can find Serialization queries under queries\serialization.txt
    The Serialization queries are simple queries that just grab large chunks of the data (50,000) results
    and are designed to test the performance of a system when it comes to giving you back large results over HTTP.
- You can find ORDER BY + DISTINCT queries under queries\order-distinct.txt
    The ORDER BY + DISTINCT queries are designed to test how well an engine optimizes the case of an ORDER BY plus
    a DISTINCT and whether using sub-queries to force one to happen before the other makes a noticeable difference
    in performance.  These tests also cover the use of ORDER BY + REDUCED
    
Operation Mix Formats
---------------------

There are two operation mix formats currently supported - 1x and 2x.

The 1x format is the format supported by the 1x releases of this tool and is a simple text file with a .txt
extension.  Each line in the file is a path to a query file, paths may be relative and will be resolved 
treating the path to the query file as the base path.  The 1x format only supports queries in its operation mixes

API Usage
=========

For information on how to use the API programmatically please see the API document

References
==========

[1] http://docs.api.talis.com/platform-api/output-types/rdf-json
[2] http://www.revelytix.com/sites/default/files/TripleStorePerformanceTestingMethodolgy.pdf
[3] http://dbis.informatik.uni-freiburg.de/index.php?project=SP2B
[4] http://swat.cse.lehigh.edu/projects/lubm/

License
=======

Copyright 2011-2014 Cray Inc. All Rights Reserved

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

* Redistributions of source code must retain the above copyright
  notice, this list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright
  notice, this list of conditions and the following disclaimer in the
  documentation and/or other materials provided with the distribution.

* Neither the name Cray Inc. nor the names of its contributors may be
  used to endorse or promote products derived from this software
  without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

Acknowledgements
=================

SPARQL Query Benchmarker uses the the Apache Jena ARQ query engine for issuing queries 
and parsing the results - http://jena.apache.org

Uses SP2B queries under the BSD license from http://dbis.informatik.uni-freiburg.de/forschung/projekte/SP2B/

Uses LUBM queries from academic paper:

GUO, Y., PAN, Z., HEFLIN, J.. LUBM: A Benchmark for OWL Knowledge Base Systems. Web Semantics: Science, Services
and Agents on the World Wide Web, North America, 3, mar. 2011. 
Available at: <http://www.websemanticsjournal.org/index.php/ps/article/view/70/68>. Date accessed: 01 Jun. 2012.
